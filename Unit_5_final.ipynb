{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhuvan0808/Machine_learning/blob/main/Unit_5_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Web scrapping\n",
        "Web scraping is the process of collecting and parsing raw data from the Web.\n",
        "The Internet hosts perhaps the greatest source of information on the planet.\n",
        "Many disciplines, such as data science, business intelligence, and investigative\n",
        "reporting, can benefit enormously from collecting and analyzing data from websites."
      ],
      "metadata": {
        "id": "VhJvMtiLB_Ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Scrape and parse text from any website and show the below:"
      ],
      "metadata": {
        "id": "b4qm38cwFUKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beautiful Soup is a Python library that makes it easy to scrape information from web pages. It sits atop an HTML or XML parser and provides Pythonic idioms for iterating, searching, and modifying the parse tree.\n"
      ],
      "metadata": {
        "id": "jtobtOZKEPKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "import re\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import csv\n",
        "\n",
        "url = url = \"https://www.reddit.com/\"\n",
        "\n",
        "page = urlopen(url)\n",
        "\n",
        "html_bytes = page.read()\n",
        "html = html_bytes.decode(\"utf-8\")"
      ],
      "metadata": {
        "id": "ZKTOSE-xBl4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Extract Text From HTML With String Methods\n"
      ],
      "metadata": {
        "id": "q9AHUsZWBZC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the title and style of the webpage\n",
        "\n",
        "title = html.find(\"<title>\") + len(\"<title>\")\n",
        "title_end = html.find(\"</title>\")\n",
        "lang = html.find('<html lang=\"') + len('<html lang=\"')\n",
        "lang_end = html.find('\">')\n",
        "\n",
        "title_text = html[title : title_end]\n",
        "lang_text = html[lang : lang_end]\n",
        "\n",
        "print(f\"Title: \\n{title_text}\")\n",
        "print(f\"\\nLang: \\n{lang_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4H4G3r35DnQ_",
        "outputId": "4798146f-1020-4030-81f4-479e32c7d065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: \n",
            "Reddit - Dive into anything\n",
            "\n",
            "Lang: \n",
            "en-US\" class=\"theme-beta theme-light\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Extract Text From HTML With Regular Expressions"
      ],
      "metadata": {
        "id": "qykEHN9tEYMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = \"<h2>.*?</h2>\"\n",
        "matches = re.findall(pattern, html)\n",
        "\n",
        "print(f\"The sub headings in {url} are: \")\n",
        "for title in matches:\n",
        "    print(re.sub(\"<.*?>\", \"\", title))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-mOo6-WEgc3",
        "outputId": "3d0aa916-e923-4a60-aca5-3a998366e66e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sub headings in https://www.reddit.com/ are: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from urllib.request import urlopen\n",
        "\n",
        "url = \"https://www.reddit.com/\"\n",
        "page = urlopen(url)\n",
        "html = page.read().decode(\"utf-8\")\n",
        "\n",
        "pattern = \"<title.*?>.*?</title.*?>\"\n",
        "match_results = re.search(pattern, html, re.IGNORECASE)\n",
        "title = match_results.group()\n",
        "title = re.sub(\"<.*?>\", \"\", title) # Remove HTML tags\n",
        "\n",
        "print(title)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrguIuWiGiF0",
        "outputId": "e4991b39-3419-4d40-9d91-6377277b468d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reddit - Dive into anything\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Use an HTML Parser for Web Scraping in Python"
      ],
      "metadata": {
        "id": "BJDCu5lZEl3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "soup = bs(html, \"html.parser\")\n",
        "\n",
        "# Title\n",
        "print(f\"Title: \\n{soup.title.string}\", end=\"\\n\\n\\n\")\n",
        "\n",
        "# Subheading\n",
        "subs = soup.find_all(\"h2\")\n",
        "print(\"All subheadings: \")\n",
        "for sub in subs:\n",
        "    print(f\"{sub.string}\")\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "# Hyperlink titles\n",
        "links = soup.find_all(\"a\")\n",
        "print(\"All hyperlink titles: \")\n",
        "for link in links:\n",
        "    if link.string != None:\n",
        "        print(f\"{link.string}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avR24qL-EkTl",
        "outputId": "a5b2bdbc-85ba-4a29-8fbc-8ec8c82b9d2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: \n",
            "Reddit - Dive into anything\n",
            "\n",
            "\n",
            "All subheadings: \n",
            "\n",
            "\n",
            "\n",
            "All hyperlink titles: \n",
            "Log In\n",
            "Gaming\n",
            "Valheim\n",
            "Genshin Impact\n",
            "Minecraft\n",
            "Pokimane\n",
            "Halo Infinite\n",
            "Call of Duty: Warzone\n",
            "Path of Exile\n",
            "Hollow Knight: Silksong\n",
            "Escape from Tarkov\n",
            "Watch Dogs: Legion\n",
            "Sports\n",
            "NFL\n",
            "NBA\n",
            "Megan Anderson\n",
            "Atlanta Hawks\n",
            "Los Angeles Lakers\n",
            "Boston Celtics\n",
            "Arsenal F.C.\n",
            "Philadelphia 76ers\n",
            "Premier League\n",
            "UFC\n",
            "Business, Economics, and Finance\n",
            "GameStop\n",
            "Moderna\n",
            "Pfizer\n",
            "Johnson & Johnson\n",
            "AstraZeneca\n",
            "Walgreens\n",
            "Best Buy\n",
            "Novavax\n",
            "SpaceX\n",
            "Tesla\n",
            "Crypto\n",
            "Cardano\n",
            "Dogecoin\n",
            "Algorand\n",
            "Bitcoin\n",
            "Litecoin\n",
            "Basic Attention Token\n",
            "Bitcoin Cash\n",
            "Television\n",
            "The Real Housewives of Atlanta\n",
            "The Bachelor\n",
            "Sister Wives\n",
            "90 Day Fiance\n",
            "Wife Swap\n",
            "The Amazing Race Australia\n",
            "Married at First Sight\n",
            "The Real Housewives of Dallas\n",
            "My 600-lb Life\n",
            "Last Week Tonight with John Oliver\n",
            "Celebrity\n",
            "Kim Kardashian\n",
            "Doja Cat\n",
            "Iggy Azalea\n",
            "Anya Taylor-Joy\n",
            "Jamie Lee Curtis\n",
            "Natalie Portman\n",
            "Henry Cavill\n",
            "Millie Bobby Brown\n",
            "Tom Hiddleston\n",
            "Keanu Reeves\n",
            "More Topics\n",
            "Animals and Pets\n",
            "Anime\n",
            "Art\n",
            "Cars and Motor Vehicles\n",
            "Crafts and DIY\n",
            "Culture, Race, and Ethnicity\n",
            "Ethics and Philosophy\n",
            "Fashion\n",
            "Food and Drink\n",
            "History\n",
            "Hobbies\n",
            "Law\n",
            "Learning and Education\n",
            "Military\n",
            "Movies\n",
            "Music\n",
            "Place\n",
            "Podcasts and Streamers\n",
            "Politics\n",
            "Programming\n",
            "Reading, Writing, and Literature\n",
            "Religion and Spirituality\n",
            "Science\n",
            "Tabletop Games\n",
            "Travel\n",
            "Japan, just Japan.\n",
            "My Pillow CEO Mike Lindell ordered to follow through with $5 million payment to expert who debunked his false election data | CNN Politics\n",
            "If you have ever walked out of a cinema because the film was so bad, what one was it?\n",
            "This is really getting out of hand...\n",
            "This store owner was continually getting broken into so he built an entry-only trap at the rear of his shop. Once the CCTV alarm alerts him, the police take care of anyone he's trapped.\n",
            "Video\n",
            "Wow, she really went off-brand.\n",
            "Russians start taking Latvian language exams. Upon failure to pass Latvian exams, Russian citizens are in danger of being expelled as of December 2\n",
            "News\n",
            "Electronic military draft notices sent out in St Petersburg, Russia\n",
            "Russia/Ukraine\n",
            "User Agreement\n",
            "Privacy policy\n",
            "Content policy\n",
            "Moderator Code of Conduct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Save the scrapped text to a text file"
      ],
      "metadata": {
        "id": "dnOCu7ueEsq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = [i.string + \"\\n\" for i in links if i.string != None]\n",
        "b = [i.string + \"\\n\" for i in subs]\n",
        "\n",
        "with open(\"output.txt\", \"w\") as file:\n",
        "    file.writelines(a)\n",
        "    # file.write(\"\\n\")\n",
        "    file.writelines(b)\n",
        "    # file.write(\"\\n\")\n",
        "    file.write(soup.title.string)\n",
        "\n",
        "print(\"Successfully written\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBtxVAheFBPi",
        "outputId": "f242acb9-5926-40f9-ea2b-29bee31f3cdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully written\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Save the scrapped text to a csv file\n"
      ],
      "metadata": {
        "id": "cyF6UEbZFF3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = [i.string for i in links if i.string != None]\n",
        "b = [i.string for i in subs]\n",
        "\n",
        "print(a)\n",
        "\n",
        "with open(\"csvout.csv\", \"w\") as file:\n",
        "    csvwriter = csv.writer(file)\n",
        "    csvwriter.writerow(a)\n",
        "    csvwriter.writerow(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RDO7a7uFMAj",
        "outputId": "021f7014-30e4-4209-a446-a8fa816367a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Log In', 'Gaming', 'Valheim', 'Genshin Impact', 'Minecraft', 'Pokimane', 'Halo Infinite', 'Call of Duty: Warzone', 'Path of Exile', 'Hollow Knight: Silksong', 'Escape from Tarkov', 'Watch Dogs: Legion', 'Sports', 'NFL', 'NBA', 'Megan Anderson', 'Atlanta Hawks', 'Los Angeles Lakers', 'Boston Celtics', 'Arsenal F.C.', 'Philadelphia 76ers', 'Premier League', 'UFC', 'Business, Economics, and Finance', 'GameStop', 'Moderna', 'Pfizer', 'Johnson & Johnson', 'AstraZeneca', 'Walgreens', 'Best Buy', 'Novavax', 'SpaceX', 'Tesla', 'Crypto', 'Cardano', 'Dogecoin', 'Algorand', 'Bitcoin', 'Litecoin', 'Basic Attention Token', 'Bitcoin Cash', 'Television', 'The Real Housewives of Atlanta', 'The Bachelor', 'Sister Wives', '90 Day Fiance', 'Wife Swap', 'The Amazing Race Australia', 'Married at First Sight', 'The Real Housewives of Dallas', 'My 600-lb Life', 'Last Week Tonight with John Oliver', 'Celebrity', 'Kim Kardashian', 'Doja Cat', 'Iggy Azalea', 'Anya Taylor-Joy', 'Jamie Lee Curtis', 'Natalie Portman', 'Henry Cavill', 'Millie Bobby Brown', 'Tom Hiddleston', 'Keanu Reeves', 'More Topics', 'Animals and Pets', 'Anime', 'Art', 'Cars and Motor Vehicles', 'Crafts and DIY', 'Culture, Race, and Ethnicity', 'Ethics and Philosophy', 'Fashion', 'Food and Drink', 'History', 'Hobbies', 'Law', 'Learning and Education', 'Military', 'Movies', 'Music', 'Place', 'Podcasts and Streamers', 'Politics', 'Programming', 'Reading, Writing, and Literature', 'Religion and Spirituality', 'Science', 'Tabletop Games', 'Travel', 'Japan, just Japan.', 'My Pillow CEO Mike Lindell ordered to follow through with $5 million payment to expert who debunked his false election data | CNN Politics', 'If you have ever walked out of a cinema because the film was so bad, what one was it?', 'This is really getting out of hand...', \"This store owner was continually getting broken into so he built an entry-only trap at the rear of his shop. Once the CCTV alarm alerts him, the police take care of anyone he's trapped.\", 'Video', 'Wow, she really went off-brand.', 'Russians start taking Latvian language exams. Upon failure to pass Latvian exams, Russian citizens are in danger of being expelled as of December 2', 'News', 'Electronic military draft notices sent out in St Petersburg, Russia', 'Russia/Ukraine', 'User Agreement', 'Privacy policy', 'Content policy', 'Moderator Code of Conduct']\n"
          ]
        }
      ]
    }
  ]
}